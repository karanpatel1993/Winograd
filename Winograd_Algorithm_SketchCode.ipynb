{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Winograd Algorithm - SketchCode.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "vK9GanOvp3a5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Project :  Automated front-end development using deep learning\n",
        "\n",
        "## Project Statement:\n",
        "Implement a Keras model to generate HTML code from hand-drawn website mockups.\n",
        "\n",
        "![](https://78.media.tumblr.com/3fc281aa001f1f17c0ce8d461cdfabeb/tumblr_nky6c83pNG1rob81ao4_r1_250.gif)\n",
        "\n",
        "\n",
        "## Intuition:\n",
        "SketchCode is a deep learning model that takes hand-drawn web mockups and converts them into working HTML code. It uses an image captioning architecture to generate its HTML markup from hand-drawn website wireframes.\n",
        "![](https://github.com/ashnkumar/sketch-code/raw/master/header_image.png)\n",
        "\n",
        "A typical design flow can quickly turn the length of the development cycle into a bottle-neck.\n",
        "![](https://cdn-images-1.medium.com/max/1000/1*zUhBTc7gJ985IGdjyinKGQ.png)\n",
        "\n",
        "Companies like Airbnb have already started to use [machine learning](https://airbnb.design/sketching-interfaces/) to make this process more efficient.\n",
        "\n",
        "This problem falls under a broader range of tasks known as [program synthesis](https://en.wikipedia.org/wiki/Program_synthesis), which is the automatic generation of working code. Another domain which we would be leveraging in our project is known as [image captioning](https://cs.stanford.edu/people/karpathy/deepimagesent/) that seeks to learn models that tie together images and text, specifically to generate descriptions of the contents of a source image.\n",
        "\n",
        "![](https://cdn-images-1.medium.com/max/1000/1*_sY92m24szeF5pFUCXWpNA.png)\n",
        "\n",
        "We will reframe our model into that of image captioning, taking a drawn website wireframe as the input image and generate its corresponding HTML code as its output text.\n",
        "\n",
        "## Getting the dataset:\n",
        "![](https://media1.giphy.com/media/bh6fRkvDxsLSw/200w.webp)\n",
        "\n",
        "For our use case, we need thousands of hand-drawn html pages which contains various bootstrap elements like buttons, dropdowns, text-boxes, etc along with their corresponding tokens. \n",
        "The [Dataset](https://github.com/tonybeltramelli/pix2code) from the pix2code paper, consists of 1,750 screenshots of synthetically generated websites and their relevant source code. The source code for each sample consists of tokens from a domain-specific-language (DSL) that the authors of the paper created for their task. Each token corresponds to a snippet of HTML and CSS, and a compiler is used to translate from the DSL to working HTML code.\n",
        "\n",
        "In order to generate hand-drawn images from the dataset, the following operations were performed:\n",
        "\n",
        "\n",
        "*   Change the border radius of elements on the page to curve the corners of buttons and divs.\n",
        "*   Adjust the thickness of borders to mimic drawn sketches, and added drop shadows.\n",
        "*   Change the font to one that looks like handwriting\n",
        "\n",
        "You can download the Dataset used for this project from [here](https://github.com/ashnkumar/sketch-code/tree/master/scripts)\n",
        "\n",
        "\n",
        "We would be adding additional bootstrap elements in our training set using the python library **opencv** which will be covered in the later section of this python notebook.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Model Architecture\n",
        "The model architecture consists of three major parts:\n",
        "* A computer vision model that uses a Convolutional Neural Network (CNN) to extract image features from the source images\n",
        "* A language model consisting of a Gated Recurrent Unit (GRU) that encodes sequences of source code tokens\n",
        "* A decoder model (also a GRU), which takes in the output from the previous two steps as its input, and predicts the next token in the sequence\n",
        "\n",
        "\n",
        "![](https://cdn-images-1.medium.com/max/1250/1*vZX3R1neqV6Rf9y_baJCYw.png)\n",
        "\n",
        "### Implementation\n",
        "\n",
        "* To train the model, we split the source code into sequences of tokens. A single input for the model is one of these sequences along with its source image, and its label is the next token in the document. The model uses the cross-entropy cost as its loss function, which compares the model’s next token prediction with the actual next token.\n",
        "\n",
        "############################################################################################################\n",
        "   ### Image\n",
        "   ![](https://github.com/karanpatel22/Machine-Learning/blob/master/01B5B64A-658F-411D-82B7-63EF0D80E8E6.png?raw=true)\n",
        "\n",
        "### Sequence of Tokens\n",
        "**header {\n",
        "btn-inactive, btn-inactive, btn-inactive, btn-inactive\n",
        "}\n",
        "row {\n",
        "double {\n",
        "small-title, text, btn-orange\n",
        "}\n",
        "double {\n",
        "small-title, text, btn-orange\n",
        "}\n",
        "}\n",
        "row {\n",
        "quadruple {\n",
        "small-title, text, btn-orange\n",
        "}\n",
        "quadruple {\n",
        "small-title, text, btn-orange\n",
        "}\n",
        "quadruple {\n",
        "small-title, text, btn-orange\n",
        "}\n",
        "quadruple {\n",
        "small-title, text, btn-orange\n",
        "}\n",
        "}\n",
        "row {\n",
        "single {\n",
        "small-title, text, btn-orange\n",
        "}\n",
        "}**\n",
        "\n",
        "############################################################################################################\n",
        "\n",
        "* At inference time when the model is tasked with generating code from scratch, the process is slightly different. The image is still processed through the CNN network, but the text process is seeded with just a starting sequence. At each step, the model’s prediction for the next token in the sequence is appended to the current input sequence, and fed into the model as a new input sequence. This is repeated until the model predicts an <END> token or the process reaches a predefined limit to the number of tokens per document.\n",
        "  \n",
        "### Example of predicted sequence of tokens\n",
        "\n",
        "\\[' < START \\>', 'btn-inactive', 'btn-inactive', 'row', 'row', '{', 'quadruple', '{', 'small-title', ',', 'text', ',', 'btn-orange', '}', 'quadruple', '{', 'small-title', ',', 'text', ',', 'btn-orange', '}', 'quadruple', '{', 'small-title', ',', 'text', ',', 'btn-orange', '}', 'quadruple', '{', 'small-title', ',', 'text', ',', 'btn-orange', '}', '}', 'row', '{', < END >']\n",
        "\n",
        "* Once the set of predicted tokens is generated from the model, a compiler converts the DSL tokens into HTML, which can be rendered in any browser.**Refer the file 'default-dsl-mapping.json' in the folder structure for the compiler which converts tokens into HTML**\n",
        "\n",
        "\n",
        "# BLEU Score\n",
        "\n",
        "We will be using BLEU score to evaluate the model. This is a common metric used in machine translation tasks, which seeks to measure how closely a machine-generated text resembles what a human would have generated, given the same input.\n",
        "\n",
        "Essentially, the BLEU compares n-gram sequences of both the generated text and reference text to create a modified form of precision. It’s very suitable for this project since it factors in the actual elements in the generated HTML, as well as where they are in relation to each other.\n",
        "\n",
        "A perfect BLEU score of 1.0 would have the right elements in the right locations given the source image, while a lower score would predict the wrong elements and/or put them in the wrong locations relative to each other. The final model was able to get a BLEU score of 0.76 on the evaluation set.\n",
        "\n",
        "# BONUS: Scaling the model to add additional bootstrap elements\n",
        "* We used the python library '**open-cv**' to add a 'dropdown' element randomly to our training dataset.\n",
        "### Example:\n",
        "![](https://github.com/karanpatel22/Machine-Learning/blob/master/dropdown.png?raw=true)\n",
        "\n",
        "* Add an appropriate **token-html mapping** for the dropdown element in the file -  **'default-dsl-mapping.json'**\n",
        "\n",
        "### Code Snippet\n",
        ". . . . . . . . . . . . . . \n",
        "\n",
        "\"btn-inactive\": \"< li >< a href=\\\"#\\\">[]< /a >< /li >\\n\",\n",
        "\n",
        "  \"dropdown\": \"< li class=\\\"dropdown\\\" > < a href=\\\"#\\\" class=\\\"dropdown-toggle\\\" data-toggle=\\\"dropdown\\\" >[]< span class=\\\"caret\\\" >< /span >< /a >< ul class=\\\"dropdown-menu\\\" role=\\\"menu\\\">< li >< a href=\\\"#tab1\\\" data-toggle=\\\"tab\\\">[]< /a >< /li >< li >< a href=\\\"#tab2\\\" data-toggle=\\\"tab\\\">[]< /a >< /li >< /ul >< /li >\\n\",\n",
        "  \n",
        "  \"row\": \"< div class=\\\"row\\\">{}< /div >\\n\",\n",
        "\n",
        ". . . . . . . . . . . . . . \n",
        "\n",
        "* Add the new bootstrap element in the **vocabulary.vocab** file.\n",
        "* Add the logic to generate random text for the **dropdown** button in the Node.rendering function().\n",
        "* Train the model and evaluate on sample images.\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "F3wYWU_cUXIV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "7346982e-a257-4c5d-c13a-112a7ad96047"
      },
      "cell_type": "code",
      "source": [
        "#Google drive authentication\n",
        "\n",
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n",
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\r\n",
            "··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "l67XSv8JqKTV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Getting the Data and relevant files\n",
        "## Download the sketch-code-master.zip folder from the given link and store it in colab."
      ]
    },
    {
      "metadata": {
        "id": "BqTe3nQqlsLx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!wget -O /content/sketch-code-master.zip 'https://www.dropbox.com/s/0b17nnfuwd1u9wc/sketch-code.zip?dl=0'\n",
        "!unzip sketch-code-master.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kU352n-WdCXE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Folder structure\n",
        "!tree sketch-code-master -I data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cEMeyTShqWOm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Class: ImagePreprocessor\n",
        "Used for performing preprocessing steps like image augmentation and resizing.\n",
        "\n",
        "# Methods:\n",
        "\n",
        "\n",
        "1.   build_image_dataset\n",
        "2.   get_img_features\n",
        "3.   save_resized_img_arrays\n",
        "4.   get_resized_images\n",
        "5.   resize_img\n",
        "\n",
        "![](https://media2.giphy.com/media/11R7nCyinm0awo/giphy.gif)"
      ]
    },
    {
      "metadata": {
        "id": "wPRe5L5gadMH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ee394828-bddb-4a88-9b20-6c2deb8a4419"
      },
      "cell_type": "code",
      "source": [
        "#ImagePreprocessor.py\n",
        "from __future__ import absolute_import\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import shutil\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import cv2\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "class ImagePreprocessor:\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def build_image_dataset(self, data_input_folder, augment_data=True):\n",
        "\n",
        "        print(\"Converting images from {} into arrays, augmentation: {}\".format(data_input_folder, augment_data))\n",
        "        resized_img_arrays, sample_ids = self.get_resized_images(data_input_folder)\n",
        "\n",
        "        if augment_data == 1:\n",
        "            self.augment_and_save_images(resized_img_arrays, sample_ids, data_input_folder)\n",
        "        else:\n",
        "            self.save_resized_img_arrays(resized_img_arrays, sample_ids, data_input_folder)\n",
        "\n",
        "    def get_img_features(self, png_path):\n",
        "        img_features = self.resize_img(png_path)\n",
        "        assert(img_features.shape == (256,256,3))\n",
        "        return img_features\n",
        "\n",
        "\n",
        "   ##########################################\n",
        "   ####### PRIVATE METHODS ##################\n",
        "   ##########################################\n",
        "\n",
        "\n",
        "\n",
        "    def save_resized_img_arrays(self, resized_img_arrays, sample_ids, output_folder):\n",
        "        count = 0\n",
        "        for img_arr, sample_id in zip(resized_img_arrays, sample_ids):\n",
        "            npz_filename = \"{}/{}.npz\".format(output_folder, sample_id)\n",
        "            np.savez_compressed(npz_filename, features=img_arr)\n",
        "            retrieve = np.load(npz_filename)[\"features\"]\n",
        "            assert np.array_equal(img_arr, retrieve)\n",
        "            count += 1\n",
        "        print(\"Saved down {} resized images to folder {}\".format(count, output_folder))\n",
        "        del resized_img_arrays\n",
        "\n",
        "    def augment_and_save_images(self, resized_img_arrays, sample_ids, data_input_folder):\n",
        "        datagen = ImageDataGenerator(\n",
        "                                 rotation_range=2,\n",
        "                                 width_shift_range=0.05,\n",
        "                                 height_shift_range=0.05,\n",
        "                                 zoom_range=0.05\n",
        "                                )\n",
        "        keras_generator = datagen.flow(resized_img_arrays,sample_ids,batch_size=1)\n",
        "        count = 0\n",
        "        for i in range(len(resized_img_arrays)):\n",
        "            img_arr, sample_id = next(keras_generator)\n",
        "            img_arr = np.squeeze(img_arr)\n",
        "            npz_filename = \"{}/{}.npz\".format(data_input_folder, sample_id[0])\n",
        "            im = Image.fromarray(img_arr.astype('uint8'))\n",
        "            np.savez_compressed(npz_filename, features=img_arr)\n",
        "            retrieve = np.load(npz_filename)[\"features\"]\n",
        "            assert np.array_equal(img_arr, retrieve)\n",
        "            count += 1\n",
        "        print(\"Saved down {} augmented images to folder {}\".format(count, data_input_folder))\n",
        "        del resized_img_arrays\n",
        "\n",
        "    def get_resized_images(self, pngs_input_folder):\n",
        "        all_files = os.listdir(pngs_input_folder)\n",
        "        png_files = [f for f in all_files if f.find(\".png\") != -1]\n",
        "        images = []\n",
        "        labels = []\n",
        "        for png_file_path in png_files:\n",
        "            png_path = \"{}/{}\".format(pngs_input_folder, png_file_path)\n",
        "            sample_id = png_file_path[:png_file_path.find('.png')]\n",
        "            resized_img_arr = self.resize_img(png_path)\n",
        "            images.append(resized_img_arr)\n",
        "            labels.append(sample_id)\n",
        "        return np.array(images), np.array(labels)\n",
        "\n",
        "    def resize_img(self, png_file_path):\n",
        "        img_rgb = cv2.imread(png_file_path)\n",
        "        img_grey = cv2.cvtColor(img_rgb, cv2.COLOR_BGR2GRAY)\n",
        "        img_adapted = cv2.adaptiveThreshold(img_grey, 255, cv2.ADAPTIVE_THRESH_MEAN_C,cv2.THRESH_BINARY, 101, 9)\n",
        "        img_stacked = np.repeat(img_adapted[...,None],3,axis=2)\n",
        "        resized = cv2.resize(img_stacked, (200,200), interpolation=cv2.INTER_AREA)\n",
        "        bg_img = 255 * np.ones(shape=(256,256,3))\n",
        "        bg_img[27:227, 27:227,:] = resized\n",
        "        bg_img /= 255\n",
        "        return bg_img"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "-97apYRDqaYa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Class: Dataset\n",
        "Used for performing tasks such as dividing dataset into training and validation set.\n",
        "\n",
        "# Methods:\n",
        "\n",
        "\n",
        "1.   split_datasets\n",
        "2.   split_samples\n",
        "3.   preprocess_data\n",
        "4.   load_vocab\n",
        "5.    create_generator\n",
        "6.  data_generator\n",
        "7.  process_data_for_generator\n",
        "8.  load_data\n",
        "9.  create_data_folders\n",
        "10. copy_files_to_folders\n",
        "11. delete_existing_folders\n",
        "12. populate_sample_ids\n",
        "13. get_all_id_sets\n",
        "14. split_paths\n",
        "\n",
        "![](https://media1.giphy.com/media/t1lmQR7sXeZJC/giphy.gif)"
      ]
    },
    {
      "metadata": {
        "id": "MzsaH9brYHsj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Dataset.py\n",
        "from __future__ import absolute_import\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import pdb\n",
        "import hashlib\n",
        "import numpy as np\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer, one_hot\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "\n",
        "VOCAB_FILE              = 'sketch-code-master/vocabulary.vocab'\n",
        "TRAINING_SET_NAME       = \"training_set\"\n",
        "VALIDATION_SET_NAME     = \"validation_set\"\n",
        "BATCH_SIZE              = 64\n",
        "\n",
        "class Dataset:\n",
        "\n",
        "    def __init__(self, data_input_folder, test_set_folder=None):\n",
        "        self.data_input_folder = data_input_folder\n",
        "        self.test_set_folder   = test_set_folder\n",
        "\n",
        "    def split_datasets(self, validation_split):\n",
        "        sample_ids = self.populate_sample_ids()\n",
        "        print(\"Total number of samples: \", len(sample_ids))\n",
        "\n",
        "        train_set_ids, val_set_ids, shuffled_sampled_ids = self.get_all_id_sets(validation_split, sample_ids)\n",
        "        training_path, validation_path = self.split_samples(train_set_ids, val_set_ids)\n",
        "\n",
        "        return training_path, validation_path\n",
        "\n",
        "    def split_samples(self, train_set_ids, val_set_ids):\n",
        "        training_path, validation_path = self.create_data_folders()\n",
        "        self.copy_files_to_folders(train_set_ids, training_path)\n",
        "        self.copy_files_to_folders(val_set_ids, validation_path)\n",
        "        return training_path, validation_path\n",
        "\n",
        "    def preprocess_data(self, training_path, validation_path, augment_training_data):\n",
        "        train_img_preprocessor = ImagePreprocessor()\n",
        "        train_img_preprocessor.build_image_dataset(training_path, augment_data=augment_training_data)\n",
        "        val_img_preprocessor = ImagePreprocessor()\n",
        "        val_img_preprocessor.build_image_dataset(validation_path, augment_data=0)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    ##########################################\n",
        "    ####### PRIVATE METHODS ##################\n",
        "    ##########################################\n",
        "\n",
        "    @classmethod\n",
        "    def load_vocab(cls):\n",
        "        file = open(VOCAB_FILE, 'r')\n",
        "        text = file.read().splitlines()[0]\n",
        "        file.close()\n",
        "        tokenizer = Tokenizer(filters='', split=\" \", lower=False)\n",
        "        tokenizer.fit_on_texts([text])\n",
        "        vocab_size = len(tokenizer.word_index) + 1\n",
        "        return tokenizer, vocab_size\n",
        "\n",
        "    @classmethod\n",
        "    def create_generator(cls, data_input_path, max_sequences):\n",
        "        img_features, text_features = Dataset.load_data(data_input_path)\n",
        "        total_sequences = 0\n",
        "        for text_set in text_features: total_sequences += len(text_set.split())\n",
        "        steps_per_epoch = total_sequences // BATCH_SIZE\n",
        "        tokenizer, vocab_size = Dataset.load_vocab()\n",
        "        data_gen = Dataset.data_generator(text_features, img_features, max_sequences, tokenizer, vocab_size)\n",
        "        return data_gen, steps_per_epoch\n",
        "\n",
        "    @classmethod\n",
        "    def data_generator(cls, text_features, img_features, max_sequences, tokenizer, vocab_size):\n",
        "        while 1:\n",
        "            for i in range(0, len(text_features), 1):\n",
        "                Ximages, XSeq, y = list(), list(),list()\n",
        "                for j in range(i, min(len(text_features), i+1)):\n",
        "                    image = img_features[j]\n",
        "                    desc = text_features[j]\n",
        "                    in_img, in_seq, out_word = Dataset.process_data_for_generator([desc], [image], max_sequences, tokenizer, vocab_size)\n",
        "                    for k in range(len(in_img)):\n",
        "                        Ximages.append(in_img[k])\n",
        "                        XSeq.append(in_seq[k])\n",
        "                        y.append(out_word[k])\n",
        "                yield [[np.array(Ximages), np.array(XSeq)], np.array(y)]\n",
        "\n",
        "    @classmethod\n",
        "    def process_data_for_generator(cls, texts, features, max_sequences, tokenizer, vocab_size):\n",
        "        X, y, image_data = list(), list(), list()\n",
        "        sequences = tokenizer.texts_to_sequences(texts)\n",
        "        for img_no, seq in enumerate(sequences):\n",
        "            for i in range(1, len(seq)):\n",
        "                in_seq, out_seq = seq[:i], seq[i]\n",
        "                in_seq = pad_sequences([in_seq], maxlen=max_sequences)[0]\n",
        "                out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
        "                image_data.append(features[img_no])\n",
        "                X.append(in_seq[-48:])\n",
        "                y.append(out_seq)\n",
        "        return np.array(image_data), np.array(X), np.array(y)\n",
        "\n",
        "    @classmethod\n",
        "    def load_data(cls, data_input_path):\n",
        "        text = []\n",
        "        images = []\n",
        "        all_filenames = os.listdir(data_input_path)\n",
        "        all_filenames.sort()\n",
        "        for filename in all_filenames:\n",
        "            if filename[-3:] == \"npz\":\n",
        "                image = np.load(data_input_path+'/'+filename)\n",
        "                images.append(image['features'])\n",
        "            elif filename[-3:] == 'gui':\n",
        "                file = open(data_input_path+'/'+filename, 'r')\n",
        "                texts = file.read()\n",
        "                file.close()\n",
        "                syntax = '<START> ' + texts + ' <END>'\n",
        "                syntax = ' '.join(syntax.split())\n",
        "                syntax = syntax.replace(',', ' ,')\n",
        "                text.append(syntax)\n",
        "        images = np.array(images, dtype=float)\n",
        "        return images, text\n",
        "\n",
        "    def create_data_folders(self):\n",
        "        training_path = \"{}/{}\".format(os.path.dirname(self.data_input_folder), TRAINING_SET_NAME)\n",
        "        validation_path = \"{}/{}\".format(os.path.dirname(self.data_input_folder), VALIDATION_SET_NAME)\n",
        "\n",
        "        self.delete_existing_folders(training_path)\n",
        "        self.delete_existing_folders(validation_path)\n",
        "\n",
        "        if not os.path.exists(training_path): os.makedirs(training_path)\n",
        "        if not os.path.exists(validation_path): os.makedirs(validation_path)\n",
        "        return training_path, validation_path\n",
        "\n",
        "    def copy_files_to_folders(self, sample_ids, output_folder):\n",
        "        copied_count = 0\n",
        "        for sample_id in sample_ids:\n",
        "            sample_id_png_path = \"{}/{}.png\".format(self.data_input_folder, sample_id)\n",
        "            sample_id_gui_path = \"{}/{}.gui\".format(self.data_input_folder, sample_id)\n",
        "            if os.path.exists(sample_id_png_path) and os.path.exists(sample_id_gui_path):\n",
        "                output_png_path = \"{}/{}.png\".format(output_folder, sample_id)\n",
        "                output_gui_path = \"{}/{}.gui\".format(output_folder, sample_id)\n",
        "                shutil.copyfile(sample_id_png_path, output_png_path)\n",
        "                shutil.copyfile(sample_id_gui_path, output_gui_path)\n",
        "                copied_count += 1\n",
        "        print(\"Moved {} files from {} to {}\".format(copied_count, self.data_input_folder, output_folder))\n",
        "\n",
        "    def delete_existing_folders(self, folder_to_delete):\n",
        "        if os.path.exists(folder_to_delete):\n",
        "            shutil.rmtree(folder_to_delete)\n",
        "            print(\"Deleted existing folder: {}\".format(folder_to_delete))\n",
        "\n",
        "    def populate_sample_ids(self):\n",
        "        all_sample_ids = []\n",
        "        full_path = self.data_input_folder\n",
        "        for f in os.listdir(full_path):\n",
        "            if f.find(\".gui\") != -1:\n",
        "                file_name = f[:f.find(\".gui\")]\n",
        "                if os.path.isfile(\"{}/{}.png\".format(self.data_input_folder, file_name)):\n",
        "                    all_sample_ids.append(file_name)\n",
        "        return all_sample_ids\n",
        "\n",
        "    def get_all_id_sets(self, validation_split, sample_ids):\n",
        "        np.random.shuffle(sample_ids)\n",
        "        val_count = int(validation_split * len(sample_ids))\n",
        "        train_count = len(sample_ids) - val_count\n",
        "        print(\"Splitting datasets, training samples: {}, validation samples: {}\".format(train_count, val_count))\n",
        "        train_set, val_set = self.split_paths(sample_ids, train_count, val_count)\n",
        "\n",
        "        return train_set, val_set, sample_ids\n",
        "\n",
        "    def split_paths(self, sample_ids, train_count, val_count):\n",
        "        count = 0\n",
        "        train_set = []\n",
        "        val_set = []\n",
        "        hashes = []\n",
        "        for sample_id in sample_ids:\n",
        "            f = open(\"{}/{}.gui\".format(self.data_input_folder, sample_id), 'r', encoding='utf-8')\n",
        "\n",
        "            with f:\n",
        "                chars = \"\"\n",
        "                for line in f:\n",
        "                    chars += line\n",
        "                content_hash = chars.replace(\" \", \"\").replace(\"\\n\", \"\")\n",
        "                content_hash = hashlib.sha256(content_hash.encode('utf-8')).hexdigest()\n",
        "\n",
        "                if len(val_set) == val_count:\n",
        "                    train_set.append(sample_id)\n",
        "                else:\n",
        "                    is_unique = True\n",
        "                    for h in hashes:\n",
        "                        if h is content_hash:\n",
        "                            is_unique = False\n",
        "                            break\n",
        "\n",
        "                    if is_unique:\n",
        "                        val_set.append(sample_id)\n",
        "                    else:\n",
        "                        train_set.append(sample_id)\n",
        "                \n",
        "                hashes.append(content_hash)\n",
        "        assert len(val_set) == val_count\n",
        "\n",
        "        return train_set, val_set\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NI5K7dGwqgB8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Class: ModelUtils\n",
        "# Methods:\n",
        "1. prepare_data_for_training"
      ]
    },
    {
      "metadata": {
        "id": "OT30JMV5bkdR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#ModelUtils.py\n",
        "from __future__ import absolute_import\n",
        "\n",
        "class ModelUtils:\n",
        "\n",
        "    @staticmethod\n",
        "    def prepare_data_for_training(data_input_folder, validation_split, augment_training_data):\n",
        "        dataset = Dataset(data_input_folder)\n",
        "        training_path, validation_path = dataset.split_datasets(validation_split)\n",
        "        dataset.preprocess_data(training_path, validation_path, augment_training_data)\n",
        "\n",
        "        return training_path, validation_path"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OfMFc8PoqkFp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Class: SketchCodeModel\n",
        "The model architecture which consists of a CNN, a langauge model and a decoder(GRU).\n",
        "\n",
        "# Methods:\n",
        "\n",
        "\n",
        "1.   load_model\n",
        "2.   save_model\n",
        "3.   create_model\n",
        "4.   train\n",
        "5.   construct_callbacks\n",
        "\n",
        "**Winograd Implementation**\n",
        "\n",
        "We implemented the Winograd algorithm in the CNN model:\n",
        "#### Observations:\n",
        "   \n",
        "| |Normal Convolution|Winograd Convolution|\n",
        "|:-:|:-:|:-:|\n",
        "|Parameters |139,723,686  |7,587,814 \n",
        "|Execution time | 219.33 min (approx)| 196.66 min (approx)\n",
        "\n",
        "![](https://media2.giphy.com/media/fo7URGO8QtGPToGZrk/giphy.gif)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "2uH21ULTb8kd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#SketchCodeModel.py\n",
        "from __future__ import absolute_import\n",
        "\n",
        "from keras.models import Model, Sequential, model_from_json\n",
        "from keras.callbacks import ModelCheckpoint, CSVLogger, Callback\n",
        "from keras.layers.core import Dense, Dropout, Flatten\n",
        "from keras.layers import Embedding, GRU, TimeDistributed, RepeatVector, LSTM, concatenate, Concatenate , Input, Reshape, Dense\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.optimizers import RMSprop\n",
        "\n",
        "\n",
        "MAX_LENGTH = 48\n",
        "MAX_SEQ    = 150\n",
        "Conv_master = None\n",
        "\n",
        "class SketchCodeModel():\n",
        "\n",
        "    def __init__(self, model_output_path, model_json_file=None, model_weights_file=None):\n",
        "\n",
        "        # Create model output path\n",
        "        self.model_output_path = model_output_path\n",
        "\n",
        "        # If we have an existing model json / weights, load in that model\n",
        "        if model_json_file is not None and model_weights_file is not None:\n",
        "            self.model = self.load_model(model_json_file, model_weights_file)\n",
        "            optimizer = RMSprop(lr=0.0001, clipvalue=1.0)\n",
        "            self.model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
        "            print(\"Loaded pretrained model from disk\")\n",
        "\n",
        "        # Create a new model if we don't have one\n",
        "        else:\n",
        "            self.create_model()\n",
        "            print(\"Created new model, vocab size: {}\".format(self.vocab_size))\n",
        "\n",
        "        print(self.model.summary())\n",
        "\n",
        "    def load_model(self, model_json_file, model_weights_file):\n",
        "        json_file = open(model_json_file, 'r')\n",
        "        loaded_model_json = json_file.read()\n",
        "        json_file.close()\n",
        "        loaded_model = model_from_json(loaded_model_json)\n",
        "        loaded_model.load_weights(model_weights_file)\n",
        "        return loaded_model\n",
        "      \n",
        "    def save_model(self):\n",
        "        model_json = self.model.to_json()\n",
        "        with open(\"{}/model_json.json\".format(self.model_output_path), \"w\") as json_file:\n",
        "            json_file.write(model_json)\n",
        "        self.model.save_weights(\"{}/weights.h5\".format(self.model_output_path))\n",
        "\n",
        "    def create_model(self):\n",
        "        tokenizer, vocab_size = Dataset.load_vocab()\n",
        "        self.vocab_size = vocab_size\n",
        "        \n",
        "        ## WINOGRAD IMPLEMENTATION\n",
        "        visual_input = Input(shape=(256,256,3))\n",
        "        Conv2D_1 = Conv2D(16, (3,1), activation = 'relu',padding='valid')(visual_input)\n",
        "        Conv2D_1 = Conv2D(16, (1,3), activation = 'relu',padding='valid')(Conv2D_1)\n",
        "        Conv2D_2 = Conv2D(16, (3,1), activation = 'relu',padding='same',strides = 2)(Conv2D_1)\n",
        "        Conv2D_2 = Conv2D(16, (1,3), activation = 'relu',padding='same',strides = 2)(Conv2D_2)\n",
        "        Conv2D_3 = Conv2D(32, (3,1), activation = 'relu',padding='same')(Conv2D_2)\n",
        "        Conv2D_3 = Conv2D(32, (1,3), activation = 'relu',padding='same')(Conv2D_3)\n",
        "        Conv2D_4 = Conv2D(32, (3,1), activation = 'relu',padding='same',strides = 2)(Conv2D_3)\n",
        "        Conv2D_4 = Conv2D(32, (1,3), activation = 'relu',padding='same',strides = 2)(Conv2D_4)\n",
        "        Conv2D_5 = Conv2D(64, (3,1), activation = 'relu',padding='same')(Conv2D_4)\n",
        "        Conv2D_5 = Conv2D(64, (1,3), activation = 'relu',padding='same')(Conv2D_5)\n",
        "        Conv2D_6 = Conv2D(64, (3,1), activation = 'relu',padding='same',strides = 2)(Conv2D_5)\n",
        "        Conv2D_6 = Conv2D(64, (1,3), activation = 'relu',padding='same',strides = 2)(Conv2D_6)\n",
        "        Conv2D_7 = Conv2D(128, (3,1), activation = 'relu',padding='same')(Conv2D_6)\n",
        "        Conv2D_7 = Conv2D(128, (1,3), activation = 'relu',padding='same')(Conv2D_7)\n",
        "        flat = Flatten()(Conv2D_7)\n",
        "        dense1 = Dense(1024, activation='relu')(flat)\n",
        "        dense1 = Dropout(0.3)(dense1)\n",
        "        dense2 = Dense(1024, activation='relu')(dense1)\n",
        "        dense2 = Dropout(0.3)(dense2)\n",
        "        encoded_image = RepeatVector(MAX_LENGTH)(dense2)\n",
        "        \n",
        "        \n",
        "        # Language encoder\n",
        "        language_input = Input(shape=(MAX_LENGTH,))\n",
        "        language_model = Embedding(vocab_size, 50, input_length=MAX_LENGTH, mask_zero=True)(language_input)\n",
        "        language_model = GRU(128, return_sequences=True)(language_model)\n",
        "        language_model = GRU(128, return_sequences=True)(language_model)\n",
        "\n",
        "        # Decoder\n",
        "        decoder = concatenate([encoded_image, language_model])\n",
        "        decoder = GRU(512, return_sequences=True)(decoder)\n",
        "        decoder = GRU(512, return_sequences=False)(decoder)\n",
        "        decoder = Dense(vocab_size, activation='softmax')(decoder)\n",
        "\n",
        "        # Compile the model\n",
        "        self.model = Model(inputs=[visual_input, language_input], outputs=decoder)\n",
        "        optimizer = RMSprop(lr=0.0001, clipvalue=1.0)\n",
        "        self.model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
        "\n",
        "    def train(self, training_path, validation_path, epochs):\n",
        "\n",
        "        # Setup data generators\n",
        "        training_generator, train_steps_per_epoch = Dataset.create_generator(training_path, max_sequences=MAX_SEQ)\n",
        "        validation_generator, val_steps_per_epoch = Dataset.create_generator(validation_path, max_sequences=MAX_SEQ)\n",
        "\n",
        "        # Setup model callbacks\n",
        "        callbacks_list = self.construct_callbacks(validation_path)\n",
        "\n",
        "        # Begin training\n",
        "        print(\"\\n### Starting model training ###\\n\")\n",
        "        self.model.fit_generator(generator=training_generator, validation_data=validation_generator, epochs=epochs, shuffle=False, validation_steps=val_steps_per_epoch, steps_per_epoch=train_steps_per_epoch, callbacks=callbacks_list, verbose=1)\n",
        "        print(\"\\n### Finished model training ###\\n\")\n",
        "        self.save_model()\n",
        "\n",
        "    def construct_callbacks(self, validation_path):\n",
        "        checkpoint_filepath=\"{}/\".format(self.model_output_path) + \"weights-epoch-{epoch:04d}--val_loss-{val_loss:.4f}--loss-{loss:.4f}.h5\"\n",
        "        csv_logger = CSVLogger(\"{}/training_val_losses.csv\".format(self.model_output_path))\n",
        "        checkpoint = ModelCheckpoint(checkpoint_filepath,\n",
        "                                    verbose=0,\n",
        "                                    save_weights_only=True,\n",
        "                                    save_best_only=True,\n",
        "                                    mode= 'min',\n",
        "                                    period=2)\n",
        "        callbacks_list = [checkpoint, csv_logger]\n",
        "        return callbacks_list\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rPpXVQjJqnjT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Class: Evaluator\n",
        "Used to evaluate the conversion of a sample image to HTML code. \n",
        "Current BLEU Score: **0.76**\n",
        "\n",
        "# Methods:\n",
        "1. get_sentence_bleu\n",
        "2. get_corpus_bleu\n",
        "3. load_gui_doc\n",
        "4. load_guis_from_folder\n",
        "\n",
        "![](https://media3.giphy.com/media/7SX08DeQl8WEgGxRVZ/giphy.gif)"
      ]
    },
    {
      "metadata": {
        "id": "Xzmd4M3ow4t0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Evaluator.py\n",
        "from __future__ import print_function\n",
        "from __future__ import absolute_import\n",
        "\n",
        "import pdb\n",
        "import os\n",
        "import operator\n",
        "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n",
        "\n",
        "class Evaluator:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    @classmethod\n",
        "    def get_sentence_bleu(cls, original_gui_filepath, generated_gui_filepath):\n",
        "        original_gui = Evaluator.load_gui_doc(original_gui_filepath)\n",
        "        generated_gui = Evaluator.load_gui_doc(generated_gui_filepath)\n",
        "        hypothesis = generated_gui[1:-1]\n",
        "        reference = original_gui\n",
        "        references = [reference]\n",
        "        return sentence_bleu(references, hypothesis)\n",
        "\n",
        "    @classmethod\n",
        "    def get_corpus_bleu(cls, original_guis_filepath, predicted_guis_filepath):\n",
        "        actuals, predicted = Evaluator.load_guis_from_folder(original_guis_filepath, predicted_guis_filepath)\n",
        "        regular_bleu = corpus_bleu(actuals, predicted)\n",
        "        return regular_bleu\n",
        "\n",
        "    @classmethod\n",
        "    def load_gui_doc(cls, gui_filepath):\n",
        "        file = open(gui_filepath, 'r')\n",
        "        gui = file.read()\n",
        "        file.close()\n",
        "        gui = ' '.join(gui.split())\n",
        "        gui = gui.replace(',', ' ,')\n",
        "        gui = gui.split()\n",
        "\n",
        "        # Predicted images don't have color so we normalize all buttons to btn-orange or btn-active\n",
        "        btns_to_replace = ['btn-green', 'btn-red']\n",
        "        normalized_gui = ['btn-orange' if token in btns_to_replace else token for token in gui]\n",
        "        normalized_gui = ['btn-active' if token == 'btn-inactive' else token for token in normalized_gui]\n",
        "        return normalized_gui\n",
        "\n",
        "    @classmethod\n",
        "    def load_guis_from_folder(cls, original_guis_filepath, predicted_guis_filepath):\n",
        "        actuals, predicted = list(), list()\n",
        "        all_files = os.listdir(predicted_guis_filepath)\n",
        "        all_predicted_files = os.listdir(predicted_guis_filepath)\n",
        "        all_predicted_guis = [f for f in all_predicted_files if f.find('.gui') != -1]\n",
        "        all_predicted_guis.sort()\n",
        "        guis = []\n",
        "        for f in all_predicted_guis:\n",
        "            generated_gui_filepath = \"{}/{}\".format(predicted_guis_filepath, f)\n",
        "            actual_gui_filepath = \"{}/{}\".format(original_guis_filepath, f)\n",
        "            if os.path.isfile(actual_gui_filepath):\n",
        "                predicted_gui = Evaluator.load_gui_doc(generated_gui_filepath)\n",
        "                actual_gui = Evaluator.load_gui_doc(actual_gui_filepath)\n",
        "\n",
        "                predicted.append(predicted_gui[1:-1])\n",
        "                actuals.append([actual_gui])\n",
        "        return actuals, predicted"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sN9jaEZcqscd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Class: SamplerUtils\n",
        "Used to generate random text for the bootstrap elements.\n",
        "\n",
        "# Methods:\n",
        "1. get_random_text"
      ]
    },
    {
      "metadata": {
        "id": "jMME6e4UxXy1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#SamplerUtils.py\n",
        "from __future__ import print_function\n",
        "from __future__ import absolute_import\n",
        "\n",
        "import string\n",
        "import random\n",
        "\n",
        "class SamplerUtils:\n",
        "\n",
        "    @staticmethod\n",
        "    def get_random_text(length_text=10, space_number=1, with_upper_case=True):\n",
        "        results = []\n",
        "        while len(results) < length_text:\n",
        "            char = random.choice(string.ascii_letters[:26])\n",
        "            results.append(char)\n",
        "        if with_upper_case:\n",
        "            results[0] = results[0].upper()\n",
        "\n",
        "        current_spaces = []\n",
        "        while len(current_spaces) < space_number:\n",
        "            space_pos = random.randint(2, length_text - 3)\n",
        "            if space_pos in current_spaces:\n",
        "                break\n",
        "            results[space_pos] = \" \"\n",
        "            if with_upper_case:\n",
        "                results[space_pos + 1] = results[space_pos - 1].upper()\n",
        "\n",
        "            current_spaces.append(space_pos)\n",
        "\n",
        "        return ''.join(results)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eHrG85mkqvjQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Class: Node\n",
        "Used in rendering the generated gui code to HTML.\n",
        "# Methods:\n",
        "\n",
        "\n",
        "1.   add_child\n",
        "2.   show\n",
        "3.   rendering_function\n",
        "4.   render\n"
      ]
    },
    {
      "metadata": {
        "id": "nBgpCZtFxP3e",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Node.py\n",
        "from __future__ import print_function\n",
        "from __future__ import absolute_import\n",
        "\n",
        "TEXT_PLACE_HOLDER = \"[]\"\n",
        "\n",
        "class Node:\n",
        "\n",
        "    def __init__(self, key, parent_node, content_holder):\n",
        "        self.key = key\n",
        "        self.parent = parent_node\n",
        "        self.children = []\n",
        "        self.content_holder = content_holder\n",
        "\n",
        "    def add_child(self, child):\n",
        "        self.children.append(child)\n",
        "\n",
        "    def show(self):\n",
        "        for child in self.children:\n",
        "            child.show()\n",
        "\n",
        "    def rendering_function(self, key, value):\n",
        "        if key.find(\"btn\") != -1:\n",
        "            value = value.replace(TEXT_PLACE_HOLDER, SamplerUtils.get_random_text())\n",
        "        elif key.find(\"title\") != -1:\n",
        "            value = value.replace(TEXT_PLACE_HOLDER, SamplerUtils.get_random_text(length_text=5, space_number=0))\n",
        "        elif key.find(\"text\") != -1:\n",
        "            value = value.replace(TEXT_PLACE_HOLDER,\n",
        "                                  SamplerUtils.get_random_text(length_text=56, space_number=7, with_upper_case=False))\n",
        "        return value\n",
        "\n",
        "    def render(self, mapping, rendering_function=None):\n",
        "        content = \"\"\n",
        "        for child in self.children:\n",
        "            placeholder = child.render(mapping, self.rendering_function)\n",
        "            if placeholder is None:\n",
        "                self = None\n",
        "                return\n",
        "            else:\n",
        "                content += placeholder\n",
        "\n",
        "        value = mapping.get(self.key, None)\n",
        "\n",
        "        if value is None:\n",
        "            self = None\n",
        "            return None\n",
        "\n",
        "        if rendering_function is not None:\n",
        "            value = self.rendering_function(self.key, value)\n",
        "\n",
        "        if len(self.children) != 0:\n",
        "            value = value.replace(self.content_holder, content)\n",
        "\n",
        "        return value"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KwbiMpgAqzX4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Compiler\n",
        "Used for compiling the generated tokens to their respective HTML code based on various styles : default, facebook, airbnb.\n",
        "# Methods:\n",
        "\n",
        "\n",
        "1.   get_stylesheet\n",
        "2.   compile\n"
      ]
    },
    {
      "metadata": {
        "id": "hnc1UThMw_xY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Compiler.py\n",
        "from __future__ import print_function\n",
        "from __future__ import absolute_import\n",
        "\n",
        "import os\n",
        "import json\n",
        "\n",
        "\n",
        "BASE_DIR_NAME = os.path.dirname('sketch-code-master/src/classes/inference/')\n",
        "DEFAULT_DSL_MAPPING_FILEPATH = \"{}/styles/default-dsl-mapping.json\".format(BASE_DIR_NAME)\n",
        "FACEBOOK_DSL_MAPPING_FILEPATH = \"{}/styles/facebook_dsl_mapping.json\".format(BASE_DIR_NAME)\n",
        "AIRBNB_DSL_MAPPING_FILEPATH = \"{}/styles/airbnb_dsl_mapping.json\".format(BASE_DIR_NAME)\n",
        "\n",
        "\n",
        "class Compiler:\n",
        "    def __init__(self, style):\n",
        "        style_json = self.get_stylesheet(style)\n",
        "        with open(style_json) as data_file:\n",
        "            self.dsl_mapping = json.load(data_file)\n",
        "\n",
        "        self.opening_tag = self.dsl_mapping[\"opening-tag\"]\n",
        "        self.closing_tag = self.dsl_mapping[\"closing-tag\"]\n",
        "        self.content_holder = self.opening_tag + self.closing_tag\n",
        "\n",
        "        self.root = Node(\"body\", None, self.content_holder)\n",
        "\n",
        "    def get_stylesheet(self, style):\n",
        "        if style == 'default':\n",
        "            return DEFAULT_DSL_MAPPING_FILEPATH\n",
        "        elif style == 'facebook':\n",
        "            return FACEBOOK_DSL_MAPPING_FILEPATH\n",
        "        elif style == 'airbnb':\n",
        "            return AIRBNB_DSL_MAPPING_FILEPATH\n",
        "\n",
        "    def compile(self, generated_gui):\n",
        "        dsl_file = generated_gui\n",
        "\n",
        "        #Parse fix\n",
        "        dsl_file = dsl_file[1:-1]\n",
        "        dsl_file = ' '.join(dsl_file)\n",
        "        dsl_file = dsl_file.replace('{', '{8').replace('}', '8}8')\n",
        "        dsl_file = dsl_file.replace(' ', '')\n",
        "        dsl_file = dsl_file.split('8')\n",
        "        dsl_file = list(filter(None, dsl_file))\n",
        "\n",
        "        current_parent = self.root\n",
        "        for token in dsl_file:\n",
        "            token = token.replace(\" \", \"\").replace(\"\\n\", \"\")\n",
        "\n",
        "            if token.find(self.opening_tag) != -1:\n",
        "                token = token.replace(self.opening_tag, \"\")\n",
        "                element = Node(token, current_parent, self.content_holder)\n",
        "                current_parent.add_child(element)\n",
        "                current_parent = element\n",
        "            elif token.find(self.closing_tag) != -1:\n",
        "                current_parent = current_parent.parent\n",
        "            else:\n",
        "                tokens = token.split(\",\")\n",
        "                for t in tokens:\n",
        "                    element = Node(t, current_parent, self.content_holder)\n",
        "                    current_parent.add_child(element)\n",
        "\n",
        "        output_html = self.root.render(self.dsl_mapping)\n",
        "        if output_html is None: return \"HTML Parsing Error\"\n",
        "\n",
        "        return output_html"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lj2SBmWgq44H",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Class: Sampler\n",
        "Used for generating the GUI code by predicting the sequence of tokens for a sample image.\n",
        "\n",
        "# Methods:\n",
        "\n",
        "\n",
        "1.   convert_batch_of_images\n",
        "2.   convert_single_image\n",
        "3.   load_model\n",
        "4.   generate_gui\n",
        "5.   generate_html\n",
        "6.   word_for_id\n",
        "7.   write_gui_to_disk\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "l5iaE55wwsV0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Sampler.py\n",
        "from __future__ import absolute_import\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import shutil\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "from keras.models import model_from_json\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "MAX_LENGTH = 48\n",
        "\n",
        "class Sampler:\n",
        "\n",
        "    def __init__(self, model_json_path=None, model_weights_path=None):\n",
        "        self.tokenizer, self.vocab_size = Dataset.load_vocab()\n",
        "        self.model = self.load_model(model_json_path, model_weights_path)\n",
        "\n",
        "    def convert_batch_of_images(self, output_folder, pngs_path, get_corpus_bleu, original_guis_filepath, style):\n",
        "\n",
        "        all_filenames = os.listdir(pngs_path)\n",
        "        all_filenames.sort()\n",
        "        generated_count = 0\n",
        "        for filename in all_filenames:\n",
        "            if filename.find('.png') != -1:\n",
        "                png_path = \"{}/{}\".format(pngs_path, filename)\n",
        "                try:\n",
        "                    self.convert_single_image(output_folder, png_path, print_generated_output=0, get_sentence_bleu=0, original_gui_filepath=png_path, style=style)\n",
        "                    generated_count += 1\n",
        "                except:\n",
        "                    print(\"Error with GUI / HTML generation:\", sys.exc_info()[0])\n",
        "                    print(sys.exc_info())\n",
        "                    continue\n",
        "        print(\"Generated code for {} images\".format(generated_count))\n",
        "\n",
        "        if (get_corpus_bleu == 1) and (original_guis_filepath is not None):\n",
        "            print(\"BLEU score: {}\".format(Evaluator.get_corpus_bleu(original_guis_filepath, output_folder)))\n",
        "\n",
        "    def convert_single_image(self, output_folder, png_path, print_generated_output, get_sentence_bleu, original_gui_filepath, style):\n",
        "\n",
        "        # Retrieve sample ID\n",
        "        png_filename = os.path.basename(png_path)\n",
        "        if png_filename.find('.png') == -1:\n",
        "            raise ValueError(\"Image is not a png!\")\n",
        "        sample_id = png_filename[:png_filename.find('.png')]\n",
        "\n",
        "        # Generate GUI\n",
        "        print(\"Generating code for sample ID {}\".format(sample_id))\n",
        "        generated_gui, gui_output_filepath= self.generate_gui(png_path, print_generated_output=print_generated_output, output_folder=output_folder, sample_id=sample_id)\n",
        "\n",
        "        # Generate HTML\n",
        "        generated_html = self.generate_html(generated_gui, sample_id, print_generated_output=print_generated_output, output_folder=output_folder, style=style)\n",
        "\n",
        "        # Get BLEU\n",
        "        if get_sentence_bleu == 1 and (original_gui_filepath is not None):\n",
        "            print(\"BLEU score: {}\".format(Evaluator.get_sentence_bleu(original_gui_filepath, gui_output_filepath)))\n",
        "\n",
        "\n",
        "    ##########################################\n",
        "    ####### PRIVATE METHODS ##################\n",
        "    ##########################################\n",
        "\n",
        "    def load_model(self, model_json_path, model_weights_path):\n",
        "        json_file = open(model_json_path, 'r')\n",
        "        loaded_model_json = json_file.read()\n",
        "        json_file.close()\n",
        "        loaded_model = model_from_json(loaded_model_json)\n",
        "        loaded_model.load_weights(model_weights_path)\n",
        "        print(\"\\nLoaded model from disk\")\n",
        "        return loaded_model\n",
        "\n",
        "    def generate_gui(self, png_path, print_generated_output, sample_id, output_folder):\n",
        "        test_img_preprocessor = ImagePreprocessor()\n",
        "        img_features = test_img_preprocessor.get_img_features(png_path)\n",
        "\n",
        "        in_text = '<START> '\n",
        "        photo = np.array([img_features])\n",
        "        for i in range(150):\n",
        "            sequence = self.tokenizer.texts_to_sequences([in_text])[0]\n",
        "            sequence = pad_sequences([sequence], maxlen=MAX_LENGTH)\n",
        "            yhat = self.model.predict([photo, sequence], verbose=0)\n",
        "            yhat = np.argmax(yhat)\n",
        "            word = self.word_for_id(yhat)\n",
        "            if word is None:\n",
        "                break\n",
        "            in_text += word + ' '\n",
        "            if word == '<END>':\n",
        "                break\n",
        "\n",
        "        generated_gui = in_text.split()\n",
        "\n",
        "        if print_generated_output is 1:\n",
        "            print(\"\\n=========\\nGenerated GUI code:\")\n",
        "            print(generated_gui)\n",
        "\n",
        "        gui_output_filepath = self.write_gui_to_disk(generated_gui, sample_id, output_folder)\n",
        "\n",
        "        return generated_gui, gui_output_filepath\n",
        "\n",
        "    def generate_html(self, gui_array, sample_id, print_generated_output, output_folder, style='default'):\n",
        "\n",
        "        compiler = Compiler(style)\n",
        "        compiled_website = compiler.compile(gui_array)\n",
        "\n",
        "        if print_generated_output is 1:\n",
        "            print(\"\\nCompiled HTML:\")\n",
        "            print(compiled_website)\n",
        "\n",
        "        if compiled_website != 'HTML Parsing Error':\n",
        "            output_filepath = \"{}/{}.html\".format(output_folder, sample_id)\n",
        "            with open(output_filepath, 'w') as output_file:\n",
        "                output_file.write(compiled_website)\n",
        "                print(\"Saved generated HTML to {}\".format(output_filepath))\n",
        "\n",
        "    def word_for_id(self, integer):\n",
        "        for word, index in self.tokenizer.word_index.items():\n",
        "            if index == integer:\n",
        "                return word\n",
        "        return None\n",
        "\n",
        "    def write_gui_to_disk(self, gui_array, sample_id, output_folder):\n",
        "        gui_output_filepath = \"{}/{}.gui\".format(output_folder, sample_id)\n",
        "        with open(gui_output_filepath, 'w') as out_f:\n",
        "            out_f.write(' '.join(gui_array))\n",
        "        return gui_output_filepath\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UC2xFFcOrMAJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Training the model\n",
        "Initiate training of the model from sratch or start training from a pretrained model.\n",
        "\n",
        "![](https://media2.giphy.com/media/26tPrcX6EfSj5N0HK/giphy.gif)\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "AToAzqvXF_nL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1784
        },
        "outputId": "cc70b5a5-2bb2-4d60-f458-f85e7b283170"
      },
      "cell_type": "code",
      "source": [
        "#train.py\n",
        "#!/usr/bin/env python\n",
        "from __future__ import print_function\n",
        "from __future__ import absolute_import\n",
        "\n",
        "from argparse import ArgumentParser\n",
        "\n",
        "VAL_SPLIT = 0.2\n",
        "\n",
        "def main():\n",
        "    data_input_path = 'sketch-code-master/data'  ## directory containing images and guis\n",
        "    validation_split = 0.2  ## portion of training data for validation set\n",
        "    epochs = 10  ## number of epochs to train on\n",
        "    model_output_path = 'sketch-code-master/model_output'  ## directory for saving model data \n",
        "    model_json_file = None ## pretrained model json file \n",
        "    model_weights_file = None ## pretrained model weights file\n",
        "    augment_training_data = 1 ## use Keras image augmentation on training data\n",
        "\n",
        "    # Load model\n",
        "    model = SketchCodeModel(model_output_path, model_json_file, model_weights_file)\n",
        "\n",
        "    # Create the model output path if it doesn't exist\n",
        "    if not os.path.exists(model_output_path):\n",
        "        os.makedirs(model_output_path)\n",
        "\n",
        "    # Split the datasets and save down image arrays\n",
        "    training_path, validation_path = ModelUtils.prepare_data_for_training(data_input_path, validation_split, augment_training_data)\n",
        "\n",
        "    # Begin model training\n",
        "    model.train(training_path=training_path,\n",
        "                validation_path=validation_path,\n",
        "                epochs=epochs)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Created new model, vocab size: 18\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            (None, 256, 256, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 254, 256, 16) 160         input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 254, 254, 16) 784         conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 127, 127, 16) 784         conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 64, 64, 16)   784         conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 64, 64, 32)   1568        conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 64, 64, 32)   3104        conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 32, 32, 32)   3104        conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 16, 16, 32)   3104        conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 16, 16, 64)   6208        conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 16, 16, 64)   12352       conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 8, 8, 64)     12352       conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 4, 4, 64)     12352       conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 4, 4, 128)    24704       conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 4, 4, 128)    49280       conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "flatten_2 (Flatten)             (None, 2048)         0           conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 1024)         2098176     flatten_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 1024)         0           dense_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "input_4 (InputLayer)            (None, 48)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 1024)         1049600     dropout_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "embedding_2 (Embedding)         (None, 48, 50)       900         input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 1024)         0           dense_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "gru_5 (GRU)                     (None, 48, 128)      68736       embedding_2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "repeat_vector_2 (RepeatVector)  (None, 48, 1024)     0           dropout_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "gru_6 (GRU)                     (None, 48, 128)      98688       gru_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 48, 1152)     0           repeat_vector_2[0][0]            \n",
            "                                                                 gru_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "gru_7 (GRU)                     (None, 48, 512)      2557440     concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "gru_8 (GRU)                     (None, 512)          1574400     gru_7[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 18)           9234        gru_8[0][0]                      \n",
            "==================================================================================================\n",
            "Total params: 7,587,814\n",
            "Trainable params: 7,587,814\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Total number of samples:  1700\n",
            "Splitting datasets, training samples: 1360, validation samples: 340\n",
            "Deleted existing folder: sketch-code-master/training_set\n",
            "Deleted existing folder: sketch-code-master/validation_set\n",
            "Moved 1360 files from sketch-code-master/data to sketch-code-master/training_set\n",
            "Moved 340 files from sketch-code-master/data to sketch-code-master/validation_set\n",
            "Converting images from sketch-code-master/training_set into arrays, augmentation: 1\n",
            "Saved down 1360 augmented images to folder sketch-code-master/training_set\n",
            "Converting images from sketch-code-master/validation_set into arrays, augmentation: 0\n",
            "Saved down 340 resized images to folder sketch-code-master/validation_set\n",
            "\n",
            "### Starting model training ###\n",
            "\n",
            "Epoch 1/10\n",
            "   6/1468 [..............................] - ETA: 58:55 - loss: 2.8229  "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1468/1468 [==============================] - 1260s 858ms/step - loss: 0.4923 - val_loss: 0.1028\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2/10\n",
            "1467/1468 [============================>.] - ETA: 0s - loss: 0.0919"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1468/1468 [==============================] - 1182s 805ms/step - loss: 0.0919 - val_loss: 0.0781\n",
            "Epoch 3/10\n",
            " 406/1468 [=======>......................] - ETA: 13:03 - loss: 0.0856"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1468/1468 [==============================] - 1182s 805ms/step - loss: 0.0844 - val_loss: 0.0785\n",
            "Epoch 4/10\n",
            " 148/1468 [==>...........................] - ETA: 16:26 - loss: 0.0788"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1468/1468 [==============================] - 1188s 809ms/step - loss: 0.0817 - val_loss: 0.0723\n",
            "Epoch 5/10\n",
            "  46/1468 [..............................] - ETA: 17:49 - loss: 0.0778"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1468/1468 [==============================] - 1189s 810ms/step - loss: 0.0802 - val_loss: 0.0737\n",
            "Epoch 6/10\n",
            "   6/1468 [..............................] - ETA: 18:18 - loss: 0.0695"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1468/1468 [==============================] - 1180s 804ms/step - loss: 0.0793 - val_loss: 0.0744\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 7/10\n",
            "1467/1468 [============================>.] - ETA: 0s - loss: 0.0788"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1468/1468 [==============================] - 1180s 804ms/step - loss: 0.0789 - val_loss: 0.0782\n",
            "Epoch 8/10\n",
            " 406/1468 [=======>......................] - ETA: 13:00 - loss: 0.0803"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1468/1468 [==============================] - 1186s 808ms/step - loss: 0.0809 - val_loss: 0.0761\n",
            "Epoch 9/10\n",
            " 148/1468 [==>...........................] - ETA: 16:18 - loss: 0.0799"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1468/1468 [==============================] - 1184s 807ms/step - loss: 0.0783 - val_loss: 0.0715\n",
            "Epoch 10/10\n",
            "  46/1468 [..............................] - ETA: 17:26 - loss: 0.0711"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1468/1468 [==============================] - 1178s 803ms/step - loss: 0.0778 - val_loss: 0.0728\n",
            "\n",
            "### Finished model training ###\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2Cg48oTOHrwF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Sample Input Image\n",
        "<img src=\"https://github.com/karanpatel22/Winograd/blob/master/drawn_example2.png?raw=true\">\n"
      ]
    },
    {
      "metadata": {
        "id": "S_lFUt8BMixl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# ConvertSingleImage\n",
        "\n",
        "Used to validate your model by testing it on sample images."
      ]
    },
    {
      "metadata": {
        "id": "T7yhMGnhwmTb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1297
        },
        "outputId": "6195136b-2fdc-41dd-db6a-44647844bca1"
      },
      "cell_type": "code",
      "source": [
        "#ConvertSingleImage.py\n",
        "#!/usr/bin/env python\n",
        "import sys\n",
        "import os\n",
        "from argparse import ArgumentParser\n",
        "from os.path import basename\n",
        "\n",
        "def main():\n",
        "    png_path = 'sketch-code-master/examples/drawn_example2.png' ## png filepath to convert into HTML\n",
        "    output_folder = 'sketch-code-master/output/generated-2.html' ## dir to save generated gui and html\n",
        "    model_json_file = 'sketch-code-master/model_output/model_json.json' ## trained model json file\n",
        "    model_weights_file = 'sketch-code-master/model_output/weights.h5' ## trained model weights file\n",
        "    print_generated_output = 1 ## see generated GUI output in terminal\n",
        "    print_bleu_score = 0 ## see BLEU score for single example\n",
        "    original_gui_filepath = None ## if getting BLEU score, provide original gui filepath\n",
        "    style = 'default' ## style to use for generation - default,facebook,airbnb\n",
        "\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    sampler = Sampler(model_json_path=model_json_file,model_weights_path = model_weights_file)\n",
        "    sampler.convert_single_image(output_folder, png_path=png_path, print_generated_output=print_generated_output, get_sentence_bleu=print_bleu_score, original_gui_filepath=original_gui_filepath, style=style)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  main()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loaded model from disk\n",
            "Generating code for sample ID drawn_example2\n",
            "\n",
            "=========\n",
            "Generated GUI code:\n",
            "['<START>', 'header', '{', 'btn-inactive', ',', 'btn-inactive', ',', 'btn-inactive', ',', 'btn-inactive', ',', 'btn-inactive', '}', 'row', '{', 'quadruple', '{', 'small-title', ',', 'text', ',', 'btn-orange', '}', 'quadruple', '{', 'small-title', ',', 'text', ',', 'btn-orange', '}', 'quadruple', '{', 'small-title', ',', 'text', ',', 'btn-orange', '}', 'quadruple', '{', 'small-title', ',', 'text', ',', 'btn-orange', '}', '}', 'row', '{', 'single', '{', 'small-title', ',', 'text', ',', 'btn-orange', '}', '}', 'row', '{', 'single', '{', 'small-title', ',', 'text', ',', 'btn-orange', '}', '}', '<END>']\n",
            "\n",
            "Compiled HTML:\n",
            "<html>\n",
            "  <header>\n",
            "    <meta charset=\"utf-8\">\n",
            "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n",
            "    <link rel=\"stylesheet\" href=\"https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css\" integrity=\"sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u\" crossorigin=\"anonymous\">\n",
            "<link rel=\"stylesheet\" href=\"https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap-theme.min.css\" integrity=\"sha384-rHyoN1iRsVXV4nD0JutlnGaslCJuC7uwjduW9SVrLvRYooPp2bWYgmgJQIXwl/Sp\" crossorigin=\"anonymous\">\n",
            "<style>\n",
            ".header{margin:20px 0}nav ul.nav-pills li{background-color:#333;border-radius:4px;margin-right:10px}.col-lg-3{width:24%;margin-right:1.333333%}.col-lg-6{width:49%;margin-right:2%}.col-lg-12,.col-lg-3,.col-lg-6{margin-bottom:20px;border-radius:6px;background-color:#f5f5f5;padding:20px}.row .col-lg-3:last-child,.row .col-lg-6:last-child{margin-right:0}footer{padding:20px 0;text-align:center;border-top:1px solid #bbb}\n",
            "</style>\n",
            "    <title>Scaffold</title>\n",
            "  </header>\n",
            "  <body>\n",
            "    <main class=\"container\">\n",
            "      <div class=\"header clearfix\">\n",
            "  <nav>\n",
            "    <ul class=\"nav nav-pills pull-left\">\n",
            "      <li><a href=\"#\">Tpk Kdobug</a></li>\n",
            "<li><a href=\"#\">Fqv Vmxmsw</a></li>\n",
            "<li><a href=\"#\">Rcn Nbkrcq</a></li>\n",
            "<li><a href=\"#\">Tmmz Zqpzo</a></li>\n",
            "<li><a href=\"#\">Qhywvit Ta</a></li>\n",
            "\n",
            "    </ul>\n",
            "  </nav>\n",
            "</div>\n",
            "<div class=\"row\"><div class=\"col-lg-3\">\n",
            "<h4>Mvdjd</h4><p>kubfwqkooezc  et ddaaznhjtz  woy k nmjpwesicmlflglsizbxl</p>\n",
            "<a class=\"btn btn-warning\" href=\"#\" role=\"button\">Cgqsbsu Up</a>\n",
            "\n",
            "</div>\n",
            "<div class=\"col-lg-3\">\n",
            "<h4>Uxugs</h4><p>crpxh twux xoayeveclxgjtkvgui q gsjbt esbjruzigiag wg qg</p>\n",
            "<a class=\"btn btn-warning\" href=\"#\" role=\"button\">Srbtvr Rhy</a>\n",
            "\n",
            "</div>\n",
            "<div class=\"col-lg-3\">\n",
            "<h4>Ahupb</h4><p>gykhd usiirvgeszzabc vxshsjrje pthzd n vad oca njmsbckyc</p>\n",
            "<a class=\"btn btn-warning\" href=\"#\" role=\"button\">Lxihlj Jty</a>\n",
            "\n",
            "</div>\n",
            "<div class=\"col-lg-3\">\n",
            "<h4>Eeytt</h4><p>oi xvmucsxm eauqygxsmrjzvkjgm abeesncjfmjrqyzaiadnxc ewr</p>\n",
            "<a class=\"btn btn-warning\" href=\"#\" role=\"button\">Qessa Awhr</a>\n",
            "\n",
            "</div>\n",
            "</div>\n",
            "<div class=\"row\"><div class=\"col-lg-12\">\n",
            "<h4>Ikcds</h4><p>muschyduzdxryyonzxjmahup pumluwhpw olb jnllgkout blzouyp</p>\n",
            "<a class=\"btn btn-warning\" href=\"#\" role=\"button\">Wee Eziwoc</a>\n",
            "\n",
            "</div>\n",
            "</div>\n",
            "<div class=\"row\"><div class=\"col-lg-12\">\n",
            "<h4>Whncn</h4><p>yd riofjs jxqvqitlv rox  t bppcdwf rogztqsdmnsraporqimgp</p>\n",
            "<a class=\"btn btn-warning\" href=\"#\" role=\"button\">Ogd Dkpunc</a>\n",
            "\n",
            "</div>\n",
            "</div>\n",
            "\n",
            " </main>\n",
            "  <script src=\"js/jquery.min.js\"></script>\n",
            "    <script src=\"js/bootstrap.min.js\"></script>\n",
            "  </body>\n",
            "</html>\n",
            "\n",
            "Saved generated HTML to sketch-code-master/output/generated-2.html/drawn_example2.html\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BIwrfuPAIoTo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Output Image\n",
        "<img src=\"https://github.com/karanpatel22/Winograd/blob/master/output.PNG?raw=true\">"
      ]
    }
  ]
}